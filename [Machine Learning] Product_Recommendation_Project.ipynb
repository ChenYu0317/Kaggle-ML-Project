{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Santander Bank Product Recommendation | Kaggle\n",
    "[Kaggle_Link](https://www.kaggle.com/c/santander-product-recommendation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "train = pd.read_csv(r'C:/Users/user/Desktop/train_ver2.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing - Part1：Handling Missing Value (Mode、Unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['ind_empleado'].fillna(train['ind_empleado'].value_counts().idxmax(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['pais_residencia'].fillna(train['pais_residencia'].value_counts().idxmax(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['sexo'].fillna('UNKNOWN', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['ult_fec_cli_1t'].fillna('UNKNOWN', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['tiprel_1mes'].fillna('A', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['indresi'].fillna(train['indresi'].value_counts().idxmax(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['indext'].fillna(train['indext'].value_counts().idxmax(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['conyuemp'].fillna(train['conyuemp'].value_counts().idxmax(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['indfall'].fillna(train['indfall'].value_counts().idxmax(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['nomprov'].fillna(train['nomprov'].value_counts().idxmax(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['ind_nomina_ult1'].fillna(train['ind_nomina_ult1'].value_counts().idxmax(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['ind_nom_pens_ult1'].fillna(train['ind_nom_pens_ult1'].value_counts().idxmax(), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_dict = { 1.0  : '1',\n",
    "            '1.0' : '1',\n",
    "            '1'   : '1',\n",
    "            2.0   : '2',\n",
    "            '2.0' : '2',\n",
    "            '2'   : '2',\n",
    "            3.0   : '3',\n",
    "            '3.0' : '3',\n",
    "            '3'   : '3',\n",
    "            'P'   : 'P',\n",
    "            4.0   : '4',\n",
    "            '4.0' : '4',\n",
    "            '4'   : '4',\n",
    "            np.nan: np.nan,\n",
    "            np.NaN: np.nan,\n",
    "            'NA'  : np.nan}\n",
    "train.indrel_1mes = train.indrel_1mes.apply(lambda x: map_dict.get(x,x))\n",
    "train.indrel_1mes.fillna(train['indrel_1mes'].value_counts().idxmax(), inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing - Part2：Handling Missing Value(Replacing missing values with data from other columns, such as the customer's residential area/income ratio.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomprovs = train.nomprov.unique() \n",
    "for nomprov in nomprovs:\n",
    "    renta = train[train.nomprov == nomprov].renta.mean(skipna=True) \n",
    "    train.loc[train.nomprov == nomprov, 'renta'] = train[train.nomprov == nomprov].renta.fillna(renta)\n",
    "print(train.renta.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nomprovs = train.nomprov.unique() \n",
    "for nomprov in nomprovs:\n",
    "    canal_entrada = train[train.nomprov == nomprov].canal_entrada.mode()[0] \n",
    "    train.loc[train.nomprov == nomprov, 'canal_entrada'] = train[train.nomprov == nomprov].canal_entrada.fillna(canal_entrada)\n",
    "print(train.canal_entrada.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_mean = train[train.segmento == '01 - TOP'].renta.mean(skipna = True)\n",
    "two_mean = train[train.segmento == '02 - PARTICULARES'].renta.mean(skipna = True)\n",
    "three_mean = train[train.segmento == '03 - UNIVERSITARIO'].renta.mean(skipna = True)\n",
    "print(one_mean)\n",
    "print(two_mean)\n",
    "print(three_mean)\n",
    "top_par = (one_mean + two_mean)/2\n",
    "par_uni = (three_mean + two_mean)/2\n",
    "print(top_par,par_uni)\n",
    "train.loc[train.renta < par_uni, 'segmento'] = train[train.renta < par_uni].segmento.fillna('03 - UNIVERSITARIO')\n",
    "train.loc[train.renta > top_par, 'segmento'] = train[train.renta > top_par].segmento.fillna('01 - TOP')\n",
    "train.loc[train.segmento.isnull(), 'segmento'] = train[train.segmento.isnull()].segmento.fillna('02 - PARTICULARES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train[train.fecha_alta.notnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Missing Value - Part 3 The final processing after completing missing value imputation (removing outliers, standardizing data types, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = train.drop(['tipodom','cod_prov'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.antiguedad = pd.to_numeric(train.antiguedad, errors = \"coerce\")\n",
    "train = train[train.antiguedad != -999999]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.age = pd.to_numeric(train.age, errors = \"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#刪除較無影響變數\n",
    "df1 = train.drop(['fecha_dato', 'fecha_alta', 'ult_fec_cli_1t', 'ncodpers'], axis=1)\n",
    "# print(df1.info())\n",
    "# print(df1.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conducting 'sampling' and 'converting data from int and float64' to overcome the limitations in computer performance and memory space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sampling\n",
    "df1 = df1.sample(n=5000000)\n",
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Processing continuous data by converting it into smaller data types\n",
    "df = df1.iloc[:,0:18]\n",
    "df.iloc[:,3] = df.iloc[:,3].astype(np.int16)\n",
    "df.iloc[:,4] = df.iloc[:,4].astype(np.float16)\n",
    "df.iloc[:,5] = df.iloc[:,5].astype(np.int16)\n",
    "df.iloc[:,6] = df.iloc[:,6].astype(np.float16)\n",
    "df.iloc[:,15] = df.iloc[:,15].astype(np.float16)\n",
    "df.iloc[:,16] = df.iloc[:,16].astype(np.float32)\n",
    "df2 = df.iloc[:,[3,4,5,6,15,16]]\n",
    "df2.info()\n",
    "df2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering - One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#For processing discrete data, use get_dummies to convert it into multiple columns, and then merge the resulting columns back into the original dataset\n",
    "\n",
    "df2 = pd.concat([df2, pd.get_dummies(df.iloc[:,0]).astype(np.int8)], axis=1)\n",
    "df2 = pd.concat([df2, pd.get_dummies(df.iloc[:,1]).astype(np.int8)], axis=1)\n",
    "df2 = pd.concat([df2, pd.get_dummies(df.iloc[:,2]).astype(np.int8)], axis=1)\n",
    "df2 = pd.concat([df2, pd.get_dummies(df.iloc[:,7]).astype(np.int8)], axis=1)\n",
    "df2 = pd.concat([df2, pd.get_dummies(df.iloc[:,8]).astype(np.int8)], axis=1)\n",
    "df2 = pd.concat([df2, pd.get_dummies(df.iloc[:,9]).astype(np.int8)], axis=1)\n",
    "df2 = pd.concat([df2, pd.get_dummies(df.iloc[:,10]).astype(np.int8)], axis=1)\n",
    "df2 = pd.concat([df2, pd.get_dummies(df.iloc[:,11]).astype(np.int8)], axis=1)\n",
    "df2 = pd.concat([df2, pd.get_dummies(df.iloc[:,12]).astype(np.int8)], axis=1)\n",
    "df2 = pd.concat([df2, pd.get_dummies(df.iloc[:,13]).astype(np.int8)], axis=1)\n",
    "df2 = pd.concat([df2, pd.get_dummies(df.iloc[:,14]).astype(np.int8)], axis=1)\n",
    "df2 = pd.concat([df2, pd.get_dummies(df.iloc[:,17]).astype(np.int8)], axis=1)\n",
    "\n",
    "# df2.head()\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adding the product information columns back into the dataset\n",
    "df2 = pd.concat([df2, df1.iloc[:,18:].astype(np.int8)], axis=1)\n",
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model-ready dataset\n",
    "df2.info()\n",
    "list(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df2.iloc[:,:-24]\n",
    "Y = df2.iloc[:,-24:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_multilabel_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing \n",
    "from sklearn.model_selection import cross_val_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building models (Random Forest, k-NN, Logistic Regression, Neural Networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#將前處理與特徵工程後的 X 與 Y 分成 training set 和 testing set \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, random_state=4)\n",
    "\n",
    "# print(X_train.shape, y_train.shape)\n",
    "# print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#隨機森林\n",
    "rf = RandomForestClassifier().fit(X_train, y_train)\n",
    "print(rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kNN\n",
    "ovr_knn = OneVsRestClassifier(KNeighborsClassifier(),-1).fit(X_train, y_train)\n",
    "print(ovr_knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#羅吉斯迴歸\n",
    "ovr_logist = OneVsRestClassifier(LogisticRegression(),-1).fit(X_train, y_train)\n",
    "print(ovr_logist.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#類神經\n",
    "ovr_mlp = OneVsRestClassifier(MLPClassifier(),-1).fit(X_train, y_train)\n",
    "print(ovr_mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logist = OneVsRestClassifier(LogisticRegression(),-1).fit(X, Y)\n",
    "c = logist.predict(X)\n",
    "d = pd.DataFrame(c)\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(24):\n",
    "    print(d.iloc[:,i].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier().fit(X, Y)\n",
    "a = rf.predict(X)\n",
    "b = pd.DataFrame(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(24):\n",
    "    print(b.iloc[:,i].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After standardizing the data, input it into various models for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#將資料標準化後再放入模型，比較結果看看\n",
    "X_train, X_test, y_train, y_test = train_test_split(preprocessing.scale(X), Y, random_state=4)\n",
    "\n",
    "# print(X_train.shape, y_train.shape)\n",
    "# print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#隨機森林(標準化後)\n",
    "rf = RandomForestClassifier().fit(X_train, y_train)\n",
    "print(rf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kNN(標準化後)\n",
    "ovr_knn = OneVsRestClassifier(KNeighborsClassifier(),-1).fit(X_train, y_train)\n",
    "print(ovr_knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#羅吉斯迴歸(標準化後)\n",
    "ovr_logist = OneVsRestClassifier(LogisticRegression(),-1).fit(X_train, y_train)\n",
    "print(ovr_logist.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#類神經(標準化後)\n",
    "ovr_mlp = OneVsRestClassifier(MLPClassifier(),-1).fit(X_train, y_train)\n",
    "print(ovr_mlp.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use cross-validation (before/after standardization) to see if it improves the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#隨機森林交叉驗證\n",
    "rf = RandomForestClassifier()\n",
    "scores = cross_val_score(rf, X, Y, cv=5)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#隨機森林交叉驗證(標準化後)\n",
    "rf = RandomForestClassifier()\n",
    "scores = cross_val_score(rf, preprocessing.scale(X), Y, cv=5)\n",
    "print(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#kNN交叉驗證\n",
    "knn = KNeighborsClassifier()\n",
    "scores = cross_val_score(knn, X, Y, cv=5)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kNN交叉驗證(標準化後)\n",
    "knn = KNeighborsClassifier()\n",
    "scores = cross_val_score(knn, preprocessing.scale(X), Y, cv=5)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#羅吉斯迴歸交叉驗證\n",
    "logist = LogisticRegression()\n",
    "scores = cross_val_score(logist, X, Y, cv=5)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#羅吉斯迴歸交叉驗證(標準化後)\n",
    "logist = LogisticRegression()\n",
    "scores = cross_val_score(logist, preprocessing.scale(X), Y, cv=5)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#類神經交叉驗證\n",
    "mlp = MLPClassifier()\n",
    "scores = cross_val_score(mlp, X, Y, cv=5)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#類神經交叉驗證(標準化後)\n",
    "mlp = MLPClassifier()\n",
    "scores = cross_val_score(mlp, preprocessing.scale(X), Y, cv=5)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
